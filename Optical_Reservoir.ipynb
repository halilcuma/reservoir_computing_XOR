{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae865c0",
   "metadata": {},
   "source": [
    "# Reservoir Computing\n",
    "In the optical context, the reservoir can be an optical system that transforms the input optical signals into a high-dimensional space. This system could be a photonic circuit or any other system that manipulates optical signals. The input signals could be different frequency components of a light source, different spatial modes of a light beam, or any other feature of the light that can be manipulated by the system.\n",
    "\n",
    "![](images/reservoir_schmatic.png)\n",
    "\n",
    "The readout layer can then be implemented as a linear optical device that combines the output signals of the reservoir in some way. The weights of this device are then optimized to perform a particular task, such as classifying the input signals or predicting future signals. This training process could involve a standard optimization algorithm such as stochastic gradient descent or any other algorithm that can optimize a linear model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4100498",
   "metadata": {},
   "source": [
    "# Reservoir Computing (RC)\n",
    "\n",
    "Reservoir computing (RC) is a machine-learning algorithm that aims to reduce the computational resources required for predicting time series without reducing accuracy. It consists of three main components: an input layer, a reservoir layer, and an output layer. The RC training process is faster compared to backpropagation through time.\n",
    "\n",
    "![](images/reservoir_schmatic.png)\n",
    "\n",
    "\n",
    "## Components of RC\n",
    "\n",
    "1. Input Layer: Receives input signals, denoted by $u(t) \\in \\mathbb{R}^{N_{\\text{in}}}$, where $N_{\\text{in}}$ is the dimension of the inputs.\n",
    "2. Reservoir Layer: Neurons in this layer are randomly connected. The states of the reservoir are denoted by $x(t) \\in \\mathbb{R}^{N_{\\text{res}}}$. The reservoir layer performs a nonlinear transformation on the input signals.\n",
    "3. Output Layer: Denoted by $y(t) \\in \\mathbb{R}^{N_{\\text{out}}}$, this layer combines the reservoir states to produce the outputs.\n",
    "\n",
    "## Mathematical Representation\n",
    "\n",
    "In the mathematical representation of RC, the following vector variables are defined:\n",
    "\n",
    "- Inputs: $u(t) \\in \\mathbb{R}^{N_{\\text{in}}}$\n",
    "- Reservoir States: $x(t) \\in \\mathbb{R}^{N_{\\text{res}}}$\n",
    "- Outputs: $y(t) \\in \\mathbb{R}^{N_{\\text{out}}}$\n",
    "- Teaching Signals: $y^{\\text{tc}}(t) \\in \\mathbb{R}^{N_{\\text{out}}}$\n",
    "\n",
    "The updates of the reservoir states are given by the equation:\n",
    "\n",
    "$$x(t) = \\tanh(W^{\\text{res}} x(t-1) + W^{\\text{in}} u(t))$$\n",
    "\n",
    "- $W^{\\text{in}} \\in \\mathbb{R}^{N_{\\text{res}} \\times N_{\\text{in}}}$: Weight matrix representing connections from the input layer to the reservoir layer. Elements are drawn from a uniform distribution $U(-\\rho_{\\text{in}}, \\rho_{\\text{in}})$.\n",
    "- $W^{\\text{res}} \\in \\mathbb{R}^{N_{\\text{res}} \\times N_{\\text{res}}}$: Weight matrix representing connections among the neurons in the reservoir layer. Elements are initialized by drawing values from a uniform distribution $U(-1, 1)$ and dividing them by a positive value to ensure the spectral radius of $W^{\\text{res}}$ is $\\rho_{\\text{res}}$.\n",
    "\n",
    "The outputs are obtained by:\n",
    "\n",
    "$$y(t) = W^{\\text{out}} x(t)$$\n",
    "\n",
    "- $W^{\\text{out}} \\in \\mathbb{R}^{N_{\\text{out}} \\times N_{\\text{res}}}$: Weight matrix representing connections from the reservoir layer to the output layer.\n",
    "\n",
    "## Training Process\n",
    "\n",
    "The output weight matrix $W^{\\text{out}}$ is trained in the offline learning process of RC using the pseudoinverse.\n",
    "\n",
    "The training procedure in RC involves training the output weights by minimizing the error between the predicted outputs and the target outputs. The error is typically measured using a loss function, such as the squared Euclidean distance.\n",
    "\n",
    "The objective is to minimize the following loss function:\n",
    "\n",
    "$$\\sum_{t=1}^{T} \\| y(t) - y^{\\text{tc}}(t) \\|_2^2$$\n",
    "\n",
    "where $\\| \\cdot \\|_2$ denotes the Euclidean norm.\n",
    "\n",
    "The optimization algorithm commonly used to minimize this loss function is the pseudoinverse. By applying the pseudoinverse, the optimal output weight matrix $W^{\\text{out}}$ can be obtained as follows:\n",
    "\n",
    "$$W^{\\text{out}} = (X X^T + \\beta I)^{-1} X Y^{\\text{tc}}$$\n",
    "\n",
    "where:\n",
    "- $X$ is the matrix of reservoir states $x(t)$ for all time steps, stacked vertically.\n",
    "- $Y^{\\text{tc}}$ is the matrix of target outputs $y^{\\text{tc}}(t)$ for all time steps, stacked vertically.\n",
    "- $\\beta$ is a regularization parameter.\n",
    "- $I$ is the identity matrix.\n",
    "\n",
    "Once the output weights $W^{\\text{out}}$ are obtained, they can be used to make predictions for new inputs by multiplying them with the corresponding reservoir states:\n",
    "\n",
    "$$\\hat{y}(t) = W^{\\text{out}} x(t)$$\n",
    "\n",
    "The training process aims to find the output weights that minimize the discrepancy between the predicted outputs $\\hat{y}(t)$ and the target outputs $y^{\\text{tc}}(t)$, thus enabling accurate predictions for future inputs.\n",
    "\n",
    "\n",
    "## Applications of RC\n",
    "\n",
    "RC has shown high performance in various time-series forecasting tasks, including chaotic time-series, weather prediction, wind-power generation, and finance. It has also been applied in control engineering and video processing.\n",
    "\n",
    "To develop applications for RC in edge computing, hardware implementations have been proposed. These implementations employ different variants of RC models and physical systems such as photonics, spintronics, mechanical oscillators, and analog integrated electronic circuits.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab45b7fc",
   "metadata": {},
   "source": [
    "$$x(t) = \\tanh(W^{\\text{res}} x(t-1) + W^{\\text{in}} u(t))$$\n",
    "\n",
    "- $x(t)$ represents the reservoir states at time $t$. These states capture the current state of the reservoir layer, which is a collection of neurons.\n",
    "- $W^{\\text{res}}$ is the weight matrix representing the connections among the neurons in the reservoir layer. It has dimensions $N_{\\text{res}} \\times N_{\\text{res}}$, where $N_{\\text{res}}$ is the number of neurons in the reservoir layer. The elements of $W^{\\text{res}}$ are initialized by drawing values from a uniform distribution $U(-1, 1)$ and dividing them by a positive value to ensure the spectral radius of $W^{\\text{res}}$ is $\\rho_{\\text{res}}$.\n",
    "- $x(t-1)$ denotes the reservoir states at the previous time step. It represents the state of the reservoir layer in the previous time step, and it is a vector with dimensions $N_{\\text{res}}$.\n",
    "- $W^{\\text{in}}$ is the weight matrix representing the connections from the input layer to the reservoir layer. It has dimensions $N_{\\text{res}} \\times N_{\\text{in}}$, where $N_{\\text{in}}$ is the dimension of the inputs. The elements of $W^{\\text{in}}$ are drawn from a uniform distribution $U(-\\rho_{\\text{in}}, \\rho_{\\text{in}})$, which is centered around zero.\n",
    "- $u(t)$ represents the input signals at time $t$. These signals are provided to the reservoir layer for processing and are a vector with dimensions $N_{\\text{in}}$.\n",
    "- $\\tanh(\\cdot)$ is the hyperbolic tangent function, which is a commonly used activation function in reservoir computing. It squashes the values between -1 and 1, allowing the reservoir states to capture nonlinear dynamics and information.\n",
    "\n",
    "Now, let's understand how the equation works. At each time step $t$, the equation calculates the reservoir states $x(t)$ based on the following components:\n",
    "\n",
    "1. $W^{\\text{res}} x(t-1)$: This term represents the influence of the previous reservoir states on the current states. It is the result of multiplying the weight matrix $W^{\\text{res}}$ with the previous reservoir states $x(t-1)$. This component captures the dynamics and dependencies within the reservoir layer.\n",
    "\n",
    "2. $W^{\\text{in}} u(t)$: This term represents the influence of the input signals on the current reservoir states. It is the result of multiplying the weight matrix $W^{\\text{in}}$ with the input signals $u(t)$. This component allows the input signals to interact with the reservoir layer, influencing its dynamics.\n",
    "\n",
    "3. $\\tanh(\\cdot)$: After combining the two components, the hyperbolic tangent activation function $\\tanh(\\cdot)$ is applied element-wise to the sum. This activation function introduces nonlinearity and helps in capturing complex relationships within the reservoir.\n",
    "\n",
    "The resulting output of $\\tanh(W^{\\text{res}} x(t-1) + W^{\\text{in}} u(t))$ is the updated reservoir states $x(t)$ at the current time step. These updated states are then used as the input for the next time step, allowing the reservoir layer to evolve and capture the temporal dependencies of the input signals.\n",
    "\n",
    "In summary, the equation $\\displaystyle x(t) = \\tanh(W^{\\text{res}} x(t-1) + W^{\\text{in}} u(t))$ describes the computation of the reservoir states at each time step, considering the influence of the previous reservoir states and the input signals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "878c13cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 17.956130981445312\n",
      "Epoch: 100, Loss: 0.018597645685076714\n",
      "Epoch: 200, Loss: 0.0012250368017703295\n",
      "Epoch: 300, Loss: 7.856312731746584e-05\n",
      "Epoch: 400, Loss: 5.005776074540336e-06\n",
      "Epoch: 500, Loss: 3.185481602940854e-07\n",
      "Epoch: 600, Loss: 2.0183563265163684e-08\n",
      "Epoch: 700, Loss: 1.3064864745615523e-09\n",
      "Epoch: 800, Loss: 1.1571987812430962e-10\n",
      "Epoch: 900, Loss: 3.1371349962228123e-11\n",
      "Test Output:\n",
      "[[0.0000000e+00]\n",
      " [9.9999547e-01]\n",
      " [9.9999666e-01]\n",
      " [5.1259995e-06]]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define the parameters\n",
    "N_in = 2  # Input dimension\n",
    "N_res = 50  # Reservoir size\n",
    "N_out = 1  # Output dimension\n",
    "spectral_radius = 0.9  # Spectral radius of the reservoir weight matrix\n",
    "regularization = 1e-6  # Regularization parameter\n",
    "\n",
    "# Generate random input data for XOR\n",
    "input_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
    "target_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
    "\n",
    "# Define the reservoir network\n",
    "class ReservoirNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ReservoirNet, self).__init__()\n",
    "        self.W_in = nn.Parameter(torch.FloatTensor(N_in, N_res).uniform_(-1, 1))\n",
    "        self.W_res = nn.Parameter(torch.FloatTensor(N_res, N_res).uniform_(-1, 1))\n",
    "        self.W_out = nn.Parameter(torch.FloatTensor(N_res, N_out).normal_())\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        batch_size = inputs.size(0)\n",
    "        reservoir_states = torch.zeros((batch_size, N_res))\n",
    "        for i in range(inputs.size(1)):\n",
    "            reservoir_states = torch.tanh(torch.matmul(inputs[:, i, :], self.W_in) +\n",
    "                                          torch.matmul(reservoir_states, self.W_res))\n",
    "        output = torch.matmul(reservoir_states, self.W_out)\n",
    "        return output\n",
    "\n",
    "# Create the reservoir network\n",
    "reservoir_net = ReservoirNet()\n",
    "\n",
    "# Define the loss function\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.SGD(reservoir_net.parameters(), lr=0.01)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 1000\n",
    "for epoch in range(num_epochs):\n",
    "    inputs = torch.tensor(input_data).unsqueeze(1)\n",
    "    targets = torch.tensor(target_data)\n",
    "\n",
    "    # Forward pass\n",
    "    output = reservoir_net(inputs)\n",
    "\n",
    "    # Compute the loss\n",
    "    loss = loss_fn(output, targets)\n",
    "\n",
    "    # Zero the gradients, perform backward pass and update the weights\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print(f\"Epoch: {epoch}, Loss: {loss.item()}\")\n",
    "\n",
    "# Test the trained model\n",
    "test_inputs = torch.tensor(input_data).unsqueeze(1)\n",
    "test_output = reservoir_net(test_inputs)\n",
    "print(\"Test Output:\")\n",
    "print(test_output.detach().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e4a800b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Test Output:\n",
      "[[0.99999547]]\n"
     ]
    }
   ],
   "source": [
    "new_input_data = np.array([[0.0, 1.0]], dtype=np.float32)\n",
    "# Convert the new input data to a PyTorch tensor\n",
    "new_test_inputs = torch.tensor(new_input_data).unsqueeze(1)\n",
    "\n",
    "# Pass the new test inputs through the reservoir network\n",
    "new_test_output = reservoir_net(new_test_inputs)\n",
    "\n",
    "# Print the new test output\n",
    "print(\"New Test Output:\")\n",
    "print(new_test_output.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "581fcacf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.9685831 ],\n",
       "       [-1.3166829 ],\n",
       "       [-1.2031087 ],\n",
       "       [ 1.1130447 ],\n",
       "       [ 1.6361916 ],\n",
       "       [-0.10822026],\n",
       "       [ 0.9838818 ],\n",
       "       [ 0.4544619 ],\n",
       "       [-0.58170706],\n",
       "       [-0.2119465 ],\n",
       "       [ 0.91164714],\n",
       "       [-0.18525532],\n",
       "       [-1.0318452 ],\n",
       "       [ 1.200445  ],\n",
       "       [ 0.19191289],\n",
       "       [ 1.2479038 ],\n",
       "       [ 0.48543692],\n",
       "       [-0.37919134],\n",
       "       [ 0.09498338],\n",
       "       [ 0.74180067],\n",
       "       [ 0.8423885 ],\n",
       "       [ 0.49872   ],\n",
       "       [ 0.6097889 ],\n",
       "       [ 0.01440328],\n",
       "       [-0.0365999 ],\n",
       "       [ 0.75025904],\n",
       "       [ 0.8323036 ],\n",
       "       [ 2.2983592 ],\n",
       "       [ 0.21785548],\n",
       "       [ 0.9683975 ],\n",
       "       [ 0.66555697],\n",
       "       [-1.3114055 ],\n",
       "       [-0.4079281 ],\n",
       "       [-1.1305478 ],\n",
       "       [-0.3501101 ],\n",
       "       [ 1.0888261 ],\n",
       "       [ 0.36138615],\n",
       "       [-0.05076846],\n",
       "       [-0.12476192],\n",
       "       [ 0.44679666],\n",
       "       [ 0.8424807 ],\n",
       "       [-0.03743272],\n",
       "       [-1.1167463 ],\n",
       "       [-0.3042814 ],\n",
       "       [ 0.04330188],\n",
       "       [-0.26905513],\n",
       "       [ 0.6259984 ],\n",
       "       [ 0.6541441 ],\n",
       "       [ 0.32227397],\n",
       "       [ 1.8547297 ]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_weights = reservoir_net.W_out.detach().numpy()\n",
    "final_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f842426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175ea4ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8df3b91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
